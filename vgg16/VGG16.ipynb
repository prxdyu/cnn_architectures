{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tgcxhObdxCKJ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uzIMpYKTyWtp"
      },
      "outputs": [],
      "source": [
        "class VGG16(nn.Module):\n",
        "\n",
        "  \"\"\"\n",
        "  Takes input image of size 224x224\n",
        "     Parameters:\n",
        "\n",
        "        in_channels   : no of input channels in the image 1 if input image is in grayscal 3 if input image is in color (defaults to 3)\n",
        "        num_classes   : no of classes in your classification setup (defaults to 1000)\n",
        "  \"\"\"\n",
        "\n",
        "  # defining the architecture as list\n",
        "  architecture=[64,64,'M',128,128,'M',256,256,256,'M',512,512,512,'M',512,512,512,'M']\n",
        "\n",
        "  def __init__(self,in_channels=3,num_classes=1000):\n",
        "\n",
        "    super(VGG16,self).__init__()\n",
        "    self.in_channels=in_channels\n",
        "    self.num_classes=num_classes\n",
        "    # creating convolutional layers\n",
        "    self.conv_layers=self.create_conv_layers(self.architecture)\n",
        "    # creating FC layers\n",
        "    self.fc_layers=nn.Sequential(nn.Linear(in_features=7*7*512,out_features=4096),\n",
        "                                 nn.ReLU(),\n",
        "                                 nn.Dropout(p=0.5),\n",
        "                                 nn.Linear(in_features=4096,out_features=4096),\n",
        "                                 nn.ReLU(),\n",
        "                                 nn.Dropout(p=0.5),\n",
        "                                 nn.Linear(in_features=4096,out_features=self.num_classes))\n",
        "\n",
        "\n",
        "  def create_conv_layers(self,architecture):\n",
        "\n",
        "    # defining empty list to store the layers of vgg16\n",
        "    layers=[]\n",
        "    in_channels=self.in_channels\n",
        "    # iterating through the architecture list to create layers\n",
        "    for i in architecture:\n",
        "\n",
        "      # if the i is an int then it is a conv layer having i no of kernels\n",
        "      if type(i)==int:\n",
        "        out_channels=i\n",
        "        # creating Conv layer with batchnorm and relu and appending it to the layers list\n",
        "        layers+=[nn.Conv2d(in_channels=in_channels,out_channels=out_channels,kernel_size=3,stride=1,padding=1),\n",
        "                      nn.BatchNorm2d(num_features=i),\n",
        "                      nn.ReLU()]\n",
        "        # updating the in_channel for the next layer (out_channels in the current layer ====> in_channel for next layer)\n",
        "        in_channels=i\n",
        "\n",
        "      # if i is M then it is a Max Pool layer\n",
        "      elif i==\"M\":\n",
        "        layers.append(nn.MaxPool2d(kernel_size=2,stride=2))\n",
        "\n",
        "    # creating the model using the sequential API and returning the model\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "  def forward(self,x):\n",
        "    # passing images through the convolutional layers\n",
        "    x=self.conv_layers(x)\n",
        "    # passing images through the FC layers\n",
        "    x = x.view(x.size(0), -1)\n",
        "    x=self.fc_layers(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "ts9JK6im5SMr",
        "outputId": "d7d280af-d8de-47e1-bd12-22d6c5f6dc76"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 170498071/170498071 [00:03<00:00, 42857247.21it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Define transforms to normalize the data\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Resize((224, 224))  # Resize to match VGG input size\n",
        "])\n",
        "\n",
        "# Load CIFAR training and test datasets\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainloader = DataLoader(trainset, batch_size=64,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = DataLoader(testset, batch_size=64,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "\n",
        "# Define the device for training\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Initialize the VGG model\n",
        "model = VGG16(num_classes=10).to(device)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Training the model\n",
        "for epoch in range(5):  # You can increase the number of epochs\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        inputs, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward + backward + optimize\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % 200 == 199:  # Print every 200 mini-batches\n",
        "            print('[%d, %5d] loss: %.3f' %\n",
        "                  (epoch + 1, i + 1, running_loss / 200))\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')\n",
        "\n",
        "# Evaluate the model\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data[0].to(device), data[1].to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
        "    100 * correct / total))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kVEvjmiu5ZmN"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
